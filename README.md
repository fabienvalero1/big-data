# Architecture Big Data et Streaming - Projet Buy & Rent

Ce projet est une refonte de l'application "Buy & Rent" en une architecture **Big Data & Streaming** utilisant l'architecture **Kappa**. Il permet l'analyse en temps rÃ©el d'annonces immobiliÃ¨res pour dÃ©tecter les meilleures opportunitÃ©s d'investissement.

## ğŸ— Architecture

L'architecture est entiÃ¨rement conteneurisÃ©e via Docker et se compose de 4 couches principales :

1.  **Ingestion Layer (Python Producers)** :
    *   `listings-producer` : Simule un flux d'annonces immobiliÃ¨res rÃ©aliste (via Faker) et les envoie dans Kafka (`real-estate-raw`).
    *   `georisks-ingester` : RÃ©cupÃ¨re (simule) les donnÃ©es de risques naturels (API GÃ©orisques) et les envoie dans Kafka.
    *   `financial-rates-producer` : Publie les taux d'intÃ©rÃªts actuels (Banque de France).

2.  **Messaging Layer (Apache Kafka)** :
    *   Sert de bus de donnÃ©es central et tampon persistant.
    *   Topics : `real-estate-raw`, `ref-georisques`, `financial-rates`.

3.  **Processing Layer (Apache Spark Structured Streaming)** :
    *   Consomme les annonces depuis Kafka.
    *   Enrichit les donnÃ©es avec les risques gÃ©ographiques et les taux financiers.
    *   Calcule les indicateurs financiers : RentabilitÃ© Brute, Cashflow, Score d'investissement.

4.  **Serving Layer (PostgreSQL)** :
    *   Stocke les annonces enrichies et les agrÃ©gats de marchÃ© dans un schÃ©ma en Ã©toile (Star Schema).
    *   Tables : `fact_listings`, `dim_location`, `ref_taux`, etc.

## ğŸš€ Comment lancer le projet

### PrÃ©-requis
*   Docker Desktop installÃ© et dÃ©marrÃ©.

### Lancement

1.  **DÃ©marrer l'infrastructure** :
    ```bash
    docker-compose up --build -d
    ```
    *Cette commande construit les images Python et tÃ©lÃ©charge les images Kafka, Spark, Postgres.*

2.  **VÃ©rifier que tout tourne** :
    ```bash
    docker-compose ps
    ```
    Tous les conteneurs doivent Ãªtre `Up`.

3.  **Soumettre le Job Spark** :
    Le conteneur `spark-processor` est configurÃ© pour lancer le job automatiquement. Vous pouvez suivre ses logs :
    ```bash
    docker logs -f spark-processor
    ```

4.  **VÃ©rifier les donnÃ©es dans PostgreSQL** :
    Connectez-vous Ã  la base de donnÃ©es :
    ```bash
    docker exec -it postgres psql -U admin -d buyandrent
    ```
    Puis requÃªtez les donnÃ©es traitÃ©es :
    ```sql
    SELECT * FROM fact_listings ORDER BY date_creation DESC LIMIT 10;
    ```

## ğŸ“‚ Structure du projet

```
.
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ producers/           # Scripts d'ingestion (Listings, Risques, Taux)
â”‚   â””â”€â”€ processors/          # Job Spark Streaming
â”œâ”€â”€ sql/
â”‚   â””â”€â”€ init.sql             # SchÃ©ma de base de donnÃ©es
â”œâ”€â”€ docker-compose.yml       # Orchestration
â”œâ”€â”€ Dockerfile.producers     # Image pour les scripts Python
â””â”€â”€ requirements.txt         # DÃ©pendances Python
```

## ğŸ“ Auteurs
*   Gael T (Ã‰tudiant Big Data)
